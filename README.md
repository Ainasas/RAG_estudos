# RAG com LLM pequeno

Este projeto demonstra a arquitetura **Retrieval-Augmented Generation (RAG)** para permitir que um LLM responda a perguntas baseadas **exclusivamente** em uma base de dados externa e espec√≠fica.

O objetivo foi aprender os conceitos chaves e superar o problema de **alucina√ß√£o** e a limita√ß√£o de **conhecimento est√°tico** dos LLMs com recursos de hardware limitados (utilizando apenas Colab).

### üß† Conceitos Estudados

O sistema RAG √© composto por tr√™s fases l√≥gicas integradas:

| Fase | Fun√ß√£o no Projeto | Conceito Aplicado | Ferramentas Utilizadas |
| :---: | :--- | :---: | :---: |
| **1. Indexa√ß√£o** | Transforma documentos de texto em vetores num√©ricos. | **Word Embeddings** | Sentence Transformers (MiniLM) |
| **2. Recupera√ß√£o** | Encontra os trechos de conhecimento mais semanticamente relevantes para a pergunta do usu√°rio. | **Similaridade Vetorial** | FAISS (Facebook AI Similarity Search) |
| **3. Gera√ß√£o** | Sintetiza os trechos recuperados em uma resposta coerente e natural. | **Prompt Engineering** | Flan-T5-base (Hugging Face) |

### ‚ùóResultados interessantes
- Foi preciso um refinamento por prompt bruto e uso de um LLM relativamente mais complexo como o Fla-T5 para conseguir uma resposta "Ok"
- O modelo se mostrou com dificuldades para criar uma resposta pr√≥pria baseada no contexto adquirido no index
- A falta de "Pensamento" √∫nico do LLM abre fronteiras para o uso de "Attention" para gerar respostas com valor
